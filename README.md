# PrediÃ§Ã£o de Surpresas EconÃ´micas: Pacote de Reprodutibilidade

Este repositÃ³rio contÃ©m todos os cÃ³digos, dados e documentaÃ§Ã£o necessÃ¡rios para reproduzir os resultados do artigo cientÃ­fico **"PrediÃ§Ã£o de Surpresas EconÃ´micas: Uma Abordagem de Machine Learning com IntegraÃ§Ã£o de Dados de Mercado Financeiro"**.

---

## ğŸ“‹ Estrutura do Projeto

```
â”œâ”€â”€ README.md                                    # Este arquivo
â”œâ”€â”€ requirements.txt                             # DependÃªncias Python
â”œâ”€â”€ data/                                        # Dados brutos e processados
â”‚   â”œâ”€â”€ final_data_merged_1.csv                 # Dataset final com features de mercado
â”‚   â””â”€â”€ data_description.md                     # DescriÃ§Ã£o detalhada dos dados
â”œâ”€â”€ scripts/                                     # Scripts de processamento e modelagem
â”‚   â”œâ”€â”€ 01_data_processing.py                   # Limpeza e cÃ¡lculo de retornos
â”‚   â”œâ”€â”€ 02_data_merging.py                      # JunÃ§Ã£o temporal de dados
â”‚   â”œâ”€â”€ 03_final_model_generation.py            # Treinamento e avaliaÃ§Ã£o do modelo
â”‚   â””â”€â”€ 04_feature_importance_plot.py           # GeraÃ§Ã£o de grÃ¡ficos
â”œâ”€â”€ results/                                     # Resultados do modelo
â”‚   â”œâ”€â”€ final_model_results_validated.txt       # MÃ©tricas de desempenho
â”‚   â”œâ”€â”€ feature_importance_final_validated.png  # GrÃ¡fico de importÃ¢ncia
â”‚   â””â”€â”€ roc_curve_final_validated.png           # Curva ROC
â”œâ”€â”€ paper/                                       # Artigo cientÃ­fico
â”‚   â”œâ”€â”€ Artigo_Cientifico_Revisado_Final.pdf    # VersÃ£o PDF
â”‚   â””â”€â”€ Artigo_Cientifico_Revisado_Final.md     # VersÃ£o Markdown
â””â”€â”€ notebooks/                                   # Notebooks Jupyter (opcional)
    â””â”€â”€ exploratory_analysis.ipynb              # AnÃ¡lise exploratÃ³ria
```

---

## ğŸ”§ Requisitos e InstalaÃ§Ã£o

### Requisitos de Sistema
- Python 3.11+
- 8GB RAM (mÃ­nimo)
- 2GB de espaÃ§o em disco

### InstalaÃ§Ã£o de DependÃªncias

```bash
pip install -r requirements.txt
```

### DependÃªncias Principais
- `pandas==2.3.4`
- `numpy==2.3.4`
- `lightgbm==4.5.0`
- `scikit-learn==1.6.1`
- `category-encoders==2.6.4`
- `matplotlib==3.10.0`
- `seaborn==0.13.2`

---

## ğŸ“Š DescriÃ§Ã£o dos Dados

### Dataset Principal: `final_data_merged_1.csv`

**DimensÃµes:** 40.316 linhas Ã— 60 colunas

**Colunas Principais:**

1. **IdentificaÃ§Ã£o e Temporalidade:**
   - `id`: Identificador Ãºnico do evento
   - `Data`: Data do evento (formato: YYYY-MM-DD)
   - `time`: Hora do evento (formato: HH:MM:SS)

2. **CaracterÃ­sticas do Evento:**
   - `zone`: Zona geogrÃ¡fica (e.g., USA, Europe, Asia)
   - `currency`: Moeda associada (e.g., USD, EUR, BRL)
   - `importance`: ImportÃ¢ncia do evento (low, medium, high)
   - `event`: Tipo de evento econÃ´mico (e.g., GDP, Unemployment Rate, CPI)

3. **Valores EconÃ´micos:**
   - `actual`: Valor divulgado
   - `forecast`: PrevisÃ£o de mercado
   - `previous`: Valor anterior

4. **VariÃ¡vel Alvo:**
   - `Y_Binary_Surprise`: Surpresa positiva (1) ou nÃ£o (0)

5. **Features de Mercado Financeiro (Retornos t-1):**
   - `Retorno_t_1_OuroFuturos`: Retorno do ouro no dia anterior
   - `Retorno_t_1_DowJonesIndustrialAverage`: Retorno do Dow Jones
   - `Retorno_t_1_Ibovespa`: Retorno do Ibovespa
   - `Retorno_t_1_GasÃ³leoLondresFuturos`: Retorno do gasÃ³leo
   - `Retorno_t_1_CrÃ©ditoCarbonoFuturos`: Retorno do crÃ©dito de carbono
   - `Retorno_t_1_MinÃ©riodeferrorefinado62%FeCFRFuturos`: Retorno do minÃ©rio de ferro
   - `Retorno_t_1_PrincipaisÃndicesMundiaisHoje`: Retorno de Ã­ndices globais
   - `Retorno_t_1_ÃndicesdeCommodities`: Retorno de Ã­ndices de commodities

---

## ğŸš€ Reproduzindo os Resultados

### Passo 1: Processamento de Dados HistÃ³ricos

```bash
python scripts/01_data_processing.py
```

**O que faz:**
- LÃª arquivos CSV de dados histÃ³ricos de ativos financeiros
- Calcula retornos diÃ¡rios (t-1) para cada ativo
- Salva os dados processados em `processed_historical_data.pkl`

### Passo 2: JunÃ§Ã£o Temporal de Dados

```bash
python scripts/02_data_merging.py
```

**O que faz:**
- Carrega os dados de eventos econÃ´micos
- Realiza junÃ§Ã£o temporal com os retornos de ativos (t-1)
- Garante que nÃ£o hÃ¡ vazamento de informaÃ§Ãµes (data leakage)
- Salva o dataset final em `data/final_data_merged_1.csv`

### Passo 3: Treinamento e AvaliaÃ§Ã£o do Modelo

```bash
python scripts/03_final_model_generation.py
```

**O que faz:**
- Carrega e prepara os dados (feature engineering, target encoding)
- Treina o modelo LightGBM com hiperparÃ¢metros otimizados
- Avalia o modelo no conjunto de teste
- Gera e salva:
  - MÃ©tricas de desempenho (`results/final_model_results_validated.txt`)
  - GrÃ¡fico de importÃ¢ncia de variÃ¡veis (`results/feature_importance_final_validated.png`)
  - Curva ROC (`results/roc_curve_final_validated.png`)

**Tempo estimado:** ~5 minutos

### Passo 4: GeraÃ§Ã£o de GrÃ¡ficos Adicionais (Opcional)

```bash
python scripts/04_feature_importance_plot.py
```

---

## ğŸ“ˆ Resultados Esperados

Ao executar os scripts acima, vocÃª deve obter os seguintes resultados:

| MÃ©trica | Valor Esperado |
|---------|----------------|
| **AUC Score** | **0.7485** |
| **AcurÃ¡cia** | **67.31%** |
| **Recall (Surpresa Positiva)** | **71.50%** |
| **F1-Score (Ponderado)** | **67.51%** |

**Nota:** Pequenas variaÃ§Ãµes (Â±0.5%) podem ocorrer devido a diferenÃ§as de ambiente computacional.

---

## ğŸ”¬ Metodologia

### 1. Feature Engineering

**VariÃ¡veis Temporais:**
- `day_of_week`: Dia da semana (0-6)
- `month`: MÃªs (1-12)
- `hour`: Hora do evento (0-23)

**CodificaÃ§Ã£o de ImportÃ¢ncia:**
- `importance_encoded`: low=1, medium=2, high=3

**Target Encoding:**
- Aplicado Ã s variÃ¡veis categÃ³ricas: `zone`, `currency`, `event`
- Biblioteca: `category_encoders.TargetEncoder`

### 2. Modelo: LightGBM

**HiperparÃ¢metros Otimizados:**
```python
{
    'n_estimators': 377,
    'learning_rate': 0.0138,
    'num_leaves': 20,
    'min_child_samples': 57,
    'subsample': 0.9806,
    'colsample_bytree': 0.9223,
    'scale_pos_weight': 1.3696  # Calculado automaticamente
}
```

**OtimizaÃ§Ã£o:**
- MÃ©todo: OtimizaÃ§Ã£o Bayesiana (Hyperopt)
- IteraÃ§Ãµes: 50
- MÃ©trica objetivo: AUC (maximizaÃ§Ã£o)
- ValidaÃ§Ã£o cruzada: 3 folds

### 3. DivisÃ£o de Dados

- **Treinamento:** 80% (32.252 amostras)
- **Teste:** 20% (8.064 amostras)
- **EstratificaÃ§Ã£o:** Sim (mantÃ©m proporÃ§Ã£o de classes)

---

## ğŸ“ CitaÃ§Ã£o

Se vocÃª utilizar este cÃ³digo ou dados em sua pesquisa, por favor cite:

```bibtex
@article{surpresas_economicas_2025,
  title={PrediÃ§Ã£o de Surpresas EconÃ´micas: Uma Abordagem de Machine Learning com IntegraÃ§Ã£o de Dados de Mercado Financeiro},
  author={[Autores]},
  journal={[Nome da Revista]},
  year={2025},
  volume={[Volume]},
  pages={[PÃ¡ginas]}
}
```

---

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, abra uma issue ou pull request.

---

## ğŸ“§ Contato

Para dÃºvidas ou sugestÃµes, entre em contato:
- **Email:** [seu-email@exemplo.com]
- **GitHub:** [seu-usuario]

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

---

## ğŸ™ Agradecimentos

Agradecemos Ã  [InstituiÃ§Ã£o] pelo suporte financeiro e infraestrutura computacional.

---

## ğŸ“š ReferÃªncias

1. Ke, G., et al. (2017). LightGBM: A highly efficient gradient boosting decision tree. *Advances in Neural Information Processing Systems*, 30.

2. Bergstra, J., et al. (2011). Algorithms for hyper-parameter optimization. *Advances in Neural Information Processing Systems*, 24.

3. Micci-Barreca, D. (2001). A preprocessing scheme for high-cardinality categorical attributes. *ACM SIGKDD Explorations Newsletter*, 3(1), 27-32.

---

**Ãšltima atualizaÃ§Ã£o:** Novembro de 2025
